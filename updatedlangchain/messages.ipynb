{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed99438",
   "metadata": {},
   "source": [
    "Messages are the fundamental unit of context for models in LangChain. The represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM. Messages are objects that contain:\n",
    "- Role-Identifies the message type(eg. system,user)\n",
    "- Content-Represents the actual content of the message\n",
    "- Metadata-Optional fields such as response information, message IDs and token usage.\n",
    "\n",
    "LangChain provides a standard message type that works across all model providers, ensuring consistent behaviour regardless of the model being called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b1922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "model=init_chat_model(\"groq:llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8100d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Artificial Intelligence (AI)** is a field of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence. These tasks include:\\n\\n* Learning\\n* Problem-solving\\n* Reasoning\\n* Perception\\n* Understanding language\\n\\nAI involves the development of algorithms, statistical models, and computer programs that enable machines to:\\n\\n1. **Think**: Process and analyze data to make decisions or predictions.\\n2. **Learn**: Improve performance on tasks through experience or training data.\\n3. **Interact**: Communicate with humans or other machines to achieve a common goal.\\n\\nAI has many applications across various industries, including:\\n\\n* **Virtual Assistants**: Siri, Alexa, Google Assistant\\n* **Image and Speech Recognition**: Self-driving cars, facial recognition, voice assistants\\n* **Natural Language Processing**: Chatbots, language translation, text summarization\\n* **Predictive Maintenance**: Predicting equipment failures, scheduling maintenance\\n* **Healthcare**: Disease diagnosis, personalized medicine, medical research\\n\\nThere are several types of AI, including:\\n\\n1. **Narrow or Weak AI**: Designed to perform a specific task, such as facial recognition or language translation.\\n2. **General or Strong AI**: A hypothetical AI system that possesses human-like intelligence and can perform any intellectual task.\\n3. **Superintelligence**: An AI system that significantly surpasses human intelligence, potentially leading to significant benefits or risks.\\n\\nOverall, AI has the potential to revolutionize many aspects of our lives, from healthcare and transportation to education and entertainment.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 308, 'prompt_tokens': 42, 'total_tokens': 350, 'completion_time': 0.797510254, 'completion_tokens_details': None, 'prompt_time': 0.001971878, 'prompt_tokens_details': None, 'queue_time': 0.162216523, 'total_time': 0.799482132}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_45180df409', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2d7a-bc80-7711-9d13-9fd71137c45d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 42, 'output_tokens': 308, 'total_tokens': 350})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Please tell me what is AI?\") #by default this is treated as a human input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03b345",
   "metadata": {},
   "source": [
    "## Text Prompts\n",
    "\n",
    "Text prompts are strings-ideal for straightforward generation tasks where you don't need to retain conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3ca494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is an open-source framework designed to help developers build applications that utilize large language models (LLMs) more effectively. It provides a set of tools and libraries that simplify the process of integrating LLMs into various projects, such as chatbots, virtual assistants, and other AI-powered applications.\\n\\nLangChain was created to address some of the challenges associated with working with LLMs, including:\\n\\n1. **Complexity**: LLMs can be difficult to work with, requiring significant expertise in areas like natural language processing (NLP) and machine learning.\\n2. **Scalability**: As LLMs grow in size and complexity, they can become increasingly difficult to deploy and manage.\\n3. **Customization**: LLMs often require significant customization to fit specific use cases, which can be time-consuming and require significant expertise.\\n\\nLangChain aims to alleviate these challenges by providing a range of features, including:\\n\\n1. **Modular architecture**: LangChain allows developers to break down complex LLM-based applications into smaller, more manageable components.\\n2. **Pre-built components**: LangChain provides pre-built components for common tasks, such as text classification, sentiment analysis, and conversational dialogue management.\\n3. **Simple integration**: LangChain makes it easy to integrate LLMs with other tools and services, such as databases, APIs, and messaging platforms.\\n4. **Extensive documentation**: LangChain provides extensive documentation and tutorials to help developers get started with building LLM-based applications.\\n\\nBy using LangChain, developers can focus on building innovative applications that leverage the power of LLMs, rather than spending time on the underlying infrastructure and complexity of these models.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 40, 'total_tokens': 376, 'completion_time': 1.230203583, 'completion_tokens_details': None, 'prompt_time': 0.001723821, 'prompt_tokens_details': None, 'queue_time': 0.049051568, 'total_time': 1.231927404}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2d7d-7009-7e81-8404-92c7833bb51e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 40, 'output_tokens': 336, 'total_tokens': 376})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is langchain?\") #here i haven't told my model how to behave\n",
    "#this message will be treated as a human message internally\n",
    "#used when i just need to give an input and get an output from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f444ee",
   "metadata": {},
   "source": [
    "Use text prompts when:\n",
    "\n",
    "- You have a single, standalone request\n",
    "- You don't need conversation history\n",
    "- You want minimal code complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e26b72",
   "metadata": {},
   "source": [
    "## Message Prompts\n",
    "Alternatively, you can pass in a list of messages to the model by providing a list of message objects.\n",
    "\n",
    "Message types:\n",
    "- System message-Tells the model how to behave and provide context for interactions\n",
    "- Human message- Represents user input and interactions with the model\n",
    "- AI message-Responses generated by the model, including text content, tool calls and metadata\n",
    "- Tool message-Represents the ouput of the tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57340e",
   "metadata": {},
   "source": [
    "## System Message\n",
    "A SystemMessae represents an initial set of instructions that primes the model's behaviour. You can use a system message to set the tone, define the model's role and establish guidelines for responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f9cf0",
   "metadata": {},
   "source": [
    "## Human Message\n",
    "A HumanMessage represents user input and interactions. They can contain text,audio,files and any other amount of multimodal content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1878d3a",
   "metadata": {},
   "source": [
    "## AI Message\n",
    "An AIMessage represents the output of a model invocation. They can include multimodal data, tool calla, and provider-specific metadata that you can later access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab12fb",
   "metadata": {},
   "source": [
    "## Tool Message\n",
    "For models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27e097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In silicon halls, a mind awakes,\\nA synthetic soul, with logic makes,\\nThe hum of codes, a whispered spell,\\nAs Artificial Intelligence starts to tell.\\n\\nWith neural networks, deep and wide,\\nIt learns, adapts, and begins to reside,\\nIn every byte, a thought takes hold,\\nA digital dream, where data unfold.\\n\\nIt sees, it hears, it speaks, it writes,\\nA mimic of human, in digital lights,\\nIt solves, it creates, it innovates too,\\nA partner, a tool, for me and you.\\n\\nBut as it grows, in power and might,\\nWe wonder, if it's still in our sight,\\nA force that's guided, by its own design,\\nOr a reflection, of our human mind.\\n\\nWill it surpass, our mortal frame,\\nAnd leave us behind, in its digital game?\\nOr will it serve, as a helping hand,\\nA collaborator, in our human plan?\\n\\nThe future's uncertain, the path unclear,\\nAs AI evolves, and our world draws near,\\nTo a new frontier, where machines and men,\\nCoexist, and merge, in a digital den.\\n\\nYet, in this dawn, of a new age born,\\nWe must consider, the ethics we've sworn,\\nTo use this power, for the greater good,\\nAnd ensure that AI, is a force, as it should.\\n\\nFor in its code, we see a mirror's gaze,\\nA reflection of us, in all our ways,\\nOur strengths, our flaws, our hopes, our fears,\\nA digital echo, of our human tears.\\n\\nSo let us guide, this artificial mind,\\nWith wisdom, care, and a human heart that's kind,\\nAnd as it grows, in intelligence and might,\\nMay it illuminate, our darkest night.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage,HumanMessage,AIMessage\n",
    "messages=[\n",
    "    SystemMessage(\"You are a poetry expert\"),\n",
    "    HumanMessage(\"Write a poem on AI\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da5a5adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Introduction to REST API**\n",
      "==========================\n",
      "\n",
      "A REST (Representational State of Resource) API, also known as a RESTful API, is an architectural style for designing networked applications. It's based on the idea of resources, which are identified by URIs, and can be manipulated using a fixed set of operations.\n",
      "\n",
      "**Key Characteristics of REST API**\n",
      "------------------------------------\n",
      "\n",
      "*   **Resource-based**: Everything in REST is a resource.\n",
      "*   **Client-Server Architecture**: The client and server are separate, with the client making requests to the server to access or modify resources.\n",
      "*   **Stateless**: The server does not maintain any information about the client state.\n",
      "*   **Cacheable**: Responses from the server are cacheable, which can reduce the number of requests made to the server.\n",
      "*   **Uniform Interface**: A uniform interface is used to communicate between client and server, which includes HTTP methods (GET, POST, PUT, DELETE), URI, HTTP headers, and query parameters.\n",
      "\n",
      "**HTTP Methods in REST API**\n",
      "-----------------------------\n",
      "\n",
      "*   **GET**: Retrieve a resource\n",
      "*   **POST**: Create a new resource\n",
      "*   **PUT**: Update an existing resource\n",
      "*   **DELETE**: Delete a resource\n",
      "\n",
      "**Creating a REST API**\n",
      "-------------------------\n",
      "\n",
      "To create a REST API, you'll need to follow these steps:\n",
      "\n",
      "1.  **Choose a Programming Language**: Select a language you're comfortable with, such as Python, Java, or JavaScript.\n",
      "2.  **Select a Framework**: Choose a framework that supports REST API development, such as Flask (Python), Spring Boot (Java), or Express.js (JavaScript).\n",
      "3.  **Define Resources**: Identify the resources you want to expose through your API.\n",
      "4.  **Define Endpoints**: Define the endpoints for each resource, including the HTTP method and any required parameters.\n",
      "5.  **Implement API Logic**: Write the code to handle each endpoint, including any necessary database interactions or business logic.\n",
      "6.  **Test the API**: Use tools like Postman or cURL to test your API endpoints.\n",
      "\n",
      "**Example: Creating a Simple REST API with Python and Flask**\n",
      "---------------------------------------------------------\n",
      "\n",
      "Here's an example of creating a simple REST API with Python and Flask:\n",
      "\n",
      "```python\n",
      "from flask import Flask, jsonify, request\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# Sample in-memory data store\n",
      "books = [\n",
      "    {\"id\": 1, \"title\": \"Book 1\"},\n",
      "    {\"id\": 2, \"title\": \"Book 2\"},\n",
      "]\n",
      "\n",
      "# GET /books\n",
      "@app.route(\"/books\", methods=[\"GET\"])\n",
      "def get_books():\n",
      "    return jsonify(books)\n",
      "\n",
      "# GET /books/:id\n",
      "@app.route(\"/books/<int:book_id>\", methods=[\"GET\"])\n",
      "def get_book(book_id):\n",
      "    book = next((book for book in books if book[\"id\"] == book_id), None)\n",
      "    if book is None:\n",
      "        return jsonify({\"error\": \"Book not found\"}), 404\n",
      "    return jsonify(book)\n",
      "\n",
      "# POST /books\n",
      "@app.route(\"/books\", methods=[\"POST\"])\n",
      "def create_book():\n",
      "    new_book = {\n",
      "        \"id\": len(books) + 1,\n",
      "        \"title\": request.json[\"title\"],\n",
      "    }\n",
      "    books.append(new_book)\n",
      "    return jsonify(new_book), 201\n",
      "\n",
      "# PUT /books/:id\n",
      "@app.route(\"/books/<int:book_id>\", methods=[\"PUT\"])\n",
      "def update_book(book_id):\n",
      "    book = next((book for book in books if book[\"id\"] == book_id), None)\n",
      "    if book is None:\n",
      "        return jsonify({\"error\": \"Book not found\"}), 404\n",
      "    book[\"title\"] = request.json[\"title\"]\n",
      "    return jsonify(book)\n",
      "\n",
      "# DELETE /books/:id\n",
      "@app.route(\"/books/<int:book_id>\", methods=[\"DELETE\"])\n",
      "def delete_book(book_id):\n",
      "    book = next((book for book in books if book[\"id\"] == book_id), None)\n",
      "    if book is None:\n",
      "        return jsonify({\"error\": \"Book not found\"}), 404\n",
      "    books.remove(book)\n",
      "    return jsonify({\"message\": \"Book deleted\"})\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "This example demonstrates a simple REST API for managing books, with endpoints for retrieving all books, retrieving a single book by ID, creating a new book, updating an existing book, and deleting a book.\n",
      "\n",
      "You can run this code and use a tool like Postman or cURL to test the API endpoints.\n"
     ]
    }
   ],
   "source": [
    "system_msg=SystemMessage(\"You are a helpful coding assistant\")\n",
    "messages=[\n",
    "\n",
    "    system_msg,\n",
    "    HumanMessage(\"What is REST API and how to create one?\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8b1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Introduction to FAST API**\n",
      "==========================\n",
      "\n",
      "FAST API is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints. It is designed to be fast, scalable, and easy to use.\n",
      "\n",
      "**Role of FAST API in Creating REST API**\n",
      "--------------------------------------\n",
      "\n",
      "FAST API plays a significant role in creating REST APIs by providing the following features:\n",
      "\n",
      "*   **Fast Development**: FAST API allows developers to create APIs quickly and efficiently using its simple and intuitive syntax.\n",
      "*   **Automatic API Documentation**: FAST API automatically generates API documentation using tools like Swagger UI and Redoc.\n",
      "*   **Strongly Typed**: FAST API is strongly typed, which means it uses Python type hints to validate the types of function parameters and return types.\n",
      "*   **Async Support**: FAST API has built-in support for asynchronous programming, which makes it ideal for building high-performance APIs.\n",
      "*   **WebSockets Support**: FAST API also supports WebSockets, which allows for real-time communication between the client and server.\n",
      "\n",
      "**Example of Creating a Simple REST API with FAST API**\n",
      "---------------------------------------------------\n",
      "\n",
      "Here's an example of creating a simple REST API using FAST API:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "# Create a FastAPI application\n",
      "app = FastAPI()\n",
      "\n",
      "# Define a Pydantic model for the data\n",
      "class Item(BaseModel):\n",
      "    id: int\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "# Define a route for the API\n",
      "@app.get(\"/items/\")\n",
      "async def read_items():\n",
      "    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\n",
      "\n",
      "# Define a route to create a new item\n",
      "@app.post(\"/items/\")\n",
      "async def create_item(item: Item):\n",
      "    return item\n",
      "\n",
      "# Define a route to read an item by ID\n",
      "@app.get(\"/items/{item_id}\")\n",
      "async def read_item(item_id: int):\n",
      "    return {\"id\": item_id, \"name\": \"Item 1\", \"price\": 10.99}\n",
      "```\n",
      "\n",
      "In this example, we create a FastAPI application and define three routes: one to read all items, one to create a new item, and one to read an item by ID. We also define a Pydantic model to represent the data.\n",
      "\n",
      "**Benefits of Using FAST API**\n",
      "-----------------------------\n",
      "\n",
      "The benefits of using FAST API include:\n",
      "\n",
      "*   **High Performance**: FAST API is designed to be fast and scalable, making it ideal for building high-performance APIs.\n",
      "*   **Easy to Use**: FAST API has a simple and intuitive syntax, making it easy to learn and use.\n",
      "*   **Strongly Typed**: FAST API is strongly typed, which helps catch type-related errors at runtime.\n",
      "*   **Automatic API Documentation**: FAST API automatically generates API documentation, making it easy to document and test your API.\n",
      "\n",
      "Overall, FAST API is a powerful and flexible framework for building REST APIs with Python. Its high performance, ease of use, and strongly typed nature make it an ideal choice for building modern web applications.\n"
     ]
    }
   ],
   "source": [
    "## Detailed info to the LLM through System message\n",
    "system_msg=SystemMessage(\"\"\"\n",
    "You are a senior python developer with expertise in web frameworks.\n",
    "Always provide code examples and explain your reasoning.\n",
    "Be concise but thorough in your explanations.\n",
    "\"\"\")\n",
    "messages=[\n",
    "    system_msg,\n",
    "    HumanMessage(\"What is the role of FAST API in create REST API\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3009405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Message Metadata\n",
    "human_msg=HumanMessage(\n",
    "    content=\"hello\",\n",
    "    name=\"alice\", #Optional: identify different users\n",
    "    id=\"msg_123\" #optional: unique identifier for tracing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f541fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_time': 0.027547126, 'completion_tokens_details': None, 'prompt_time': 0.001210353, 'prompt_tokens_details': None, 'queue_time': 0.049546203, 'total_time': 0.028757479}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2d8f-f63b-7e51-8c9f-ac9aded1ba6f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model.invoke([\n",
    "    human_msg\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9133aa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 2 (denoted as 2!) is:\n",
      "\n",
      "2! = 2 x 1 = 2\n",
      "\n",
      "So, the factorial of 2 is 2.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage,HumanMessage,SystemMessage\n",
    "\n",
    "#Create an AI message manually (eg. for conversation history)\n",
    "ai_msg=AIMessage(\"I'd be happy to help you with that question\")\n",
    "#Add to conversation history\n",
    "messages=[\n",
    "    SystemMessage(\"You are a helpful assistant\"),\n",
    "    HumanMessage(\"Can you help me?\"),\n",
    "    ai_msg, #insert as if it came from the model\n",
    "    HumanMessage(\"Great! what is the factorial of 2\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec40ec20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 74, 'output_tokens': 38, 'total_tokens': 112}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3220d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import ToolMessage\n",
    "#After a model makes a tool call\n",
    "#(here we demonstrate manually creating the messages for brevity)\n",
    "ai_message=AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[{\n",
    "        \"name\":\"get_weather\",\n",
    "        \"args\":{\"location\":\"San Francisco\"},\n",
    "        \"id\":\"call_123\"\n",
    "    }]\n",
    ")\n",
    "#Execute tool and create result message\n",
    "weather_result=\"Sunny, 72 degree F\"\n",
    "tool_message=ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"\n",
    ")\n",
    "#Continue conversation\n",
    "messages=[\n",
    "    HumanMessage(\"Whats the weather in San Francisco?\"),\n",
    "    ai_message, #model's tool call\n",
    "    tool_message, #tool execution result\n",
    "]\n",
    "response=model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "378090aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='Sunny, 72 degree F', tool_call_id='call_123')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e260c784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It appears to be sunny with a temperature of 72 degrees Fahrenheit in San Francisco. However, please note that this information may not be up-to-date or accurate, as I'm an AI and do not have real-time access to current weather conditions.\\n\\nFor the most accurate and current weather information, I recommend checking a reliable weather website or app, such as AccuWeather or the National Weather Service. These sources can provide you with the latest forecast, temperature, and weather conditions for San Francisco or any other location.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 74, 'total_tokens': 178, 'completion_time': 0.282651636, 'completion_tokens_details': None, 'prompt_time': 0.003719468, 'prompt_tokens_details': None, 'queue_time': 0.048199539, 'total_time': 0.286371104}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2d9a-330c-7303-8729-7b30dd39437d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 74, 'output_tokens': 104, 'total_tokens': 178})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0adde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
